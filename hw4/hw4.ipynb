{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ECON8502: Structural Microeconometrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Conor Bayliss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This week you will estimate a simple hidden Markov model using Expectation-Maximisation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here is the model. There is a discrete state variable $k\\in\\{1,2,3,...,K\\}$ and a binary outcome $j\\in\\{0,1\\}$ that:*\n",
    "1. *Is determined probabilistically by the state; and*\n",
    "2. *Moves the state up one grid point if $j=1$, i.e. $k_{t+1} = \\min\\{K,k_{t+j}\\}$*\n",
    "\n",
    "*Let $p$ be a K-dimensional vector where $p_k$ holds the probability that $j=1$ given that the model is in state $k$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state $k$ is never observed and the outcome $j$ is only observed half the time (i.e. it is missing with probability 0.5). Thus, define $j^{*}$ to be:\n",
    "$$\n",
    "j^{*} = \n",
    "\\begin{cases} \n",
    "j & \\text{with probability } 0.5 \\\\\n",
    "-1 & \\text{with probability } 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Our task is to estimate the vector of outcome probabilities $p$ non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below calcualtes $p$ and simulates panel data*\n",
    "$$\n",
    "(j^*_{nt})_{t=1,n=1}^{T,N}\n",
    "$$\n",
    "*To start with, assume $k_{n,1} = 1, \\forall n$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pj = [0.7310585786300049, 0.6224593312018546, 0.5, 0.3775406687981454, 0.2689414213699951]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×1000 Matrix{Float64}:\n",
       " -1.0  -1.0  -1.0   1.0  -1.0  -1.0  …   1.0  -1.0   1.0  -1.0  -1.0  -1.0\n",
       " -1.0  -1.0  -1.0  -1.0  -1.0  -1.0     -1.0   1.0   1.0  -1.0   1.0  -1.0\n",
       " -1.0  -1.0  -1.0   1.0  -1.0  -1.0      0.0  -1.0  -1.0  -1.0  -1.0  -1.0\n",
       "  1.0   1.0   0.0  -1.0  -1.0   1.0     -1.0  -1.0   1.0  -1.0   1.0  -1.0\n",
       " -1.0   0.0   0.0  -1.0  -1.0  -1.0      1.0  -1.0  -1.0   1.0   1.0   1.0\n",
       "  0.0  -1.0  -1.0   1.0  -1.0  -1.0  …   0.0   0.0   0.0  -1.0   0.0  -1.0\n",
       "  1.0   0.0  -1.0   1.0  -1.0   0.0      1.0  -1.0   1.0  -1.0   0.0  -1.0\n",
       "  0.0   0.0   0.0   0.0   0.0  -1.0     -1.0   1.0  -1.0  -1.0  -1.0   0.0\n",
       "  0.0  -1.0   0.0   0.0  -1.0  -1.0     -1.0   0.0  -1.0  -1.0  -1.0   1.0\n",
       " -1.0  -1.0   1.0   1.0   1.0   0.0     -1.0   1.0   0.0   0.0   0.0  -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random, Distributions\n",
    "K = 5\n",
    "Pj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #<- choice probability as a function of k\n",
    "@show Pj\n",
    "knext(k,j,K) = min(K,k+j) \n",
    "\n",
    "function simulate(N,T,Pj)\n",
    "    J = zeros(T,N)\n",
    "    K = length(Pj)\n",
    "    for n in axes(J,2)\n",
    "        k = 1\n",
    "        for t in axes(J,1)\n",
    "            j = rand()<Pj[k]\n",
    "            # record j probabilistically\n",
    "            if rand()<0.5\n",
    "                J[t,n] = -1\n",
    "            else\n",
    "                J[t,n] = j\n",
    "            end\n",
    "            # update state:\n",
    "            k = knext(k,j,K)\n",
    "        end\n",
    "    end\n",
    "    return J\n",
    "end\n",
    "\n",
    "J_data = simulate(1000,10,Pj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the outcomes $j$ are peridoically unobserved, there are two ways to set up the EM problem:*\n",
    "1. *Define a composite state variable $s=(k,j)$ that is partially unobserved and define $\\alpha$ and $\\beta$ over this composite state*\n",
    "2. *Define $\\alpha$ and $\\beta$ over $k$ only and sum over potential realisations of $j$ when missing*.\n",
    "\n",
    "*When the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since $j$ is binary, we will use the second approach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a function that (given a guess of the parameters $p$) takes a sequence of $(j^*)_{t=1}^T$ and runs the forward-back algorithm. In doing so, your function should fill in three objects:*\n",
    "1. *A $K$ x $T$ array of backward looking probabilities $(\\alpha)$*\n",
    "2. *A $K$ x $T$ array of forward looking probabilities $(\\beta)$*\n",
    "3. *A $K$ x $T$ array of posterior probabilities over each state $k(Q)$*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall that*\n",
    "$$\n",
    "\\alpha[k,s] = \\mathbb{P}[k_s=k,(j^*)_{t=1}^s]\n",
    "$$\n",
    "$$\n",
    "\\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^T|k_s=k]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "Q[k,s] = \\mathbb{P}[k_s=k|(j^*)_{t=1}^T]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5 0.0 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.03125 0.0; 0.0 0.0 … 0.96875 1.0], [3.0517578125e-5 6.103515625e-5 … 0.125 0.25; 3.0517578125e-5 6.103515625e-5 … 0.125 0.25; … ; 3.0517578125e-5 6.103515625e-5 … 0.125 0.25; 3.0517578125e-5 6.103515625e-5 … 0.125 0.25], [1.0 0.5 … 0.0 0.0; 0.0 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.0625 0.03125; 0.0 0.0 … 0.9375 0.96875])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "P_guess = zeros(K,1) \n",
    "P_guess .= 0.5\n",
    "α_init = zeros(K,1)\n",
    "α_init[1] = 1\n",
    "β_end = ones(K,1)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "function fb(P_guess, J_data, n , α_init, β_end)\n",
    "    T,N = size(J_data)\n",
    "    K = length(P_guess)\n",
    "    α, β = zeros(K,T+1), zeros(K,T+1)\n",
    "    α[:,1] .= α_init\n",
    "    β[:,T+1] .= β_end\n",
    "    Q = zeros(K,T)\n",
    "    #############################\n",
    "    for j in 1:1\n",
    "        if j == 1\n",
    "            if J_data[j,n] == 0\n",
    "                α[:,2] .= α[:,1]\n",
    "            elseif J_data[j,n] == 1\n",
    "                α[2:K,2] .= α[1:K-1,1]\n",
    "                α[K,2] += α[K,1]\n",
    "            elseif J_data[j,n] == -1\n",
    "                α[1,2] = α[1,1] * (1 - P_guess[1])\n",
    "                α[2,2] = α[1,1] * P_guess[1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    for j in 2:T\n",
    "        if J_data[j,n] == 0\n",
    "            α[:,j+1] .= α[:,j]\n",
    "        elseif J_data[j,n] == 1\n",
    "            α[2:K,j+1] = α[1:K-1,j]\n",
    "            α[K,j+1] += α[K,j] \n",
    "        elseif J_data[j,n] == -1\n",
    "            α_temp_0 = zeros(K,1)\n",
    "            α_temp_1 = zeros(K,1)\n",
    "            α_temp_0[1:K-1] = α[1:K-1,j] .* (1 .- P_guess[1:K-1])\n",
    "            α_temp_1[2:K] = α[1:K-1,j] .* P_guess[1:K-1]\n",
    "            α_temp_1[K] += α[K,j]\n",
    "            α[:,j+1] = α_temp_0 .+ α_temp_1\n",
    "        end\n",
    "    end\n",
    "    #############################\n",
    "    for j in reverse(1:T)\n",
    "        j_today = J_data[j,n]\n",
    "        if j_today == -1 \n",
    "            β[:,j] .= 0.5 .* β[:,j+1]\n",
    "        elseif j_today == 0 \n",
    "            β[:,j] .= 0.25 .* β[:,j+1]\n",
    "        elseif j_today == 1 \n",
    "            β[:,j] .= 0.25 .* β[:,j+1]\n",
    "        end\n",
    "    end\n",
    "    ###############################\n",
    "    for t in 1:T\n",
    "        Q[:,t] = α[:,t] .* β[:,t]\n",
    "        Q[:,t] = Q[:,t] ./ sum(Q[:,t])\n",
    "    end\n",
    "    ###############################\n",
    "    return α[:, end-9:end], β[:, 1:10], Q\n",
    "end\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "ex_α, ex_β, ex_Q = fb(P_guess,J_data,747,α_init,β_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a $K$ x $T$ x $N$ array of posterior weights.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes in empty matrices of size $K$ x $T$ x $N$, calculates $\\alpha$, $\\beta$ and $Q$ for all simulated paths,and fills in the 3d matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5 0.25 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.25 0.15625; 0.0 0.0 … 0.6875 0.8125;;; 0.5 0.25 … 0.0 0.0; 0.5 0.5 … 0.03125 0.015625; … ; 0.0 0.0 … 0.3125 0.234375; 0.0 0.0 … 0.5 0.65625;;; 0.5 0.25 … 0.03125 0.0; 0.5 0.5 … 0.15625 0.03125; … ; 0.0 0.0 … 0.3125 0.3125; 0.0 0.0 … 0.1875 0.5;;; … ;;; 0.5 0.25 … 0.0 0.0; 0.5 0.5 … 0.00390625 0.00390625; … ; 0.0 0.0 … 0.109375 0.109375; 0.0 0.0 … 0.85546875 0.85546875;;; 0.5 0.0 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.0625 0.0625; 0.0 0.0 … 0.9375 0.9375;;; 0.5 0.25 … 0.0 0.0; 0.5 0.5 … 0.0 0.0; … ; 0.0 0.0 … 0.09375 0.0546875; 0.0 0.0 … 0.890625 0.9375], [3.0517578125e-5 6.103515625e-5 … 0.125 0.5; 3.0517578125e-5 6.103515625e-5 … 0.125 0.5; … ; 3.0517578125e-5 6.103515625e-5 … 0.125 0.5; 3.0517578125e-5 6.103515625e-5 … 0.125 0.5;;; 6.103515625e-5 0.0001220703125 … 0.25 0.5; 6.103515625e-5 0.0001220703125 … 0.25 0.5; … ; 6.103515625e-5 0.0001220703125 … 0.25 0.5; 6.103515625e-5 0.0001220703125 … 0.25 0.5;;; 3.0517578125e-5 6.103515625e-5 … 0.0625 0.25; 3.0517578125e-5 6.103515625e-5 … 0.0625 0.25; … ; 3.0517578125e-5 6.103515625e-5 … 0.0625 0.25; 3.0517578125e-5 6.103515625e-5 … 0.0625 0.25;;; … ;;; 0.000244140625 0.00048828125 … 0.125 0.25; 0.000244140625 0.00048828125 … 0.125 0.25; … ; 0.000244140625 0.00048828125 … 0.125 0.25; 0.000244140625 0.00048828125 … 0.125 0.25;;; 1.52587890625e-5 3.0517578125e-5 … 0.125 0.25; 1.52587890625e-5 3.0517578125e-5 … 0.125 0.25; … ; 1.52587890625e-5 3.0517578125e-5 … 0.125 0.25; 1.52587890625e-5 3.0517578125e-5 … 0.125 0.25;;; 0.0001220703125 0.000244140625 … 0.125 0.5; 0.0001220703125 0.000244140625 … 0.125 0.5; … ; 0.0001220703125 0.000244140625 … 0.125 0.5; 0.0001220703125 0.000244140625 … 0.125 0.5], [1.0 1.0 … 1.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.375 0.09375; 1.0 1.0 … 0.5 0.890625;;; 1.0 1.0 … 1.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.015625 0.1640625 … 0.0625 0.1640625; 0.984375 0.7734375 … 0.9375 0.7734375;;; 1.0 1.0 … 1.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.1640625; 1.0 1.0 … 1.0 0.7734375;;; 1.0 1.0 … 1.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.0625 0.15625 … 0.25 0.0; 0.9375 0.8125 … 0.75 1.0;;; 1.0 1.0 … 1.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.03125 0.0625 … 0.015625 0.09375; 0.96875 0.9375 … 0.984375 0.890625])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "α_obs = zeros(K,10,1000)\n",
    "β_obs = zeros(K,10,1000)\n",
    "Q_obs = zeros(K,10,1000)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "function iterate(α_obs, β_obs, Q_obs, P_guess, J_data, α_init, β_end)\n",
    "    for n in 1:1000\n",
    "        α_obs[:,:,n], β_obs[:,:,n], Q_obs[:,:,n] = fb(P_guess, J_data, n, α_init, β_end)\n",
    "    end\n",
    "    Q_obs = reshape(Q_obs, (1000,10,5))\n",
    "    return α_obs, β_obs, Q_obs\n",
    "end\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "α_out, β_out, Q_out = iterate(α_obs, β_obs, Q_obs, P_guess, J_data, α_init, β_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take as given a set of posterior weights $q_{ntk}=Q[n,t,k]$. The expected log-likelihood for the M-step is:*\n",
    "$$\n",
    "\\mathcal{L}(p) = \\sum_n \\sum_t \\sum_k q_{ntk}(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_k)+\\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_k))\n",
    "$$\n",
    "*Show that the non-parametric maximum likelihood estimate of $p$ given the posterior weights is a frequency estimator:*\n",
    "$$\n",
    "\\hat{p_k} = \\frac{\\sum_n \\sum_t \\mathbf{1}\\{j^*_{nt}=1\\}q_{ntk}}{\\sum_n \\sum_t \\mathbf{1}\\{j^*_{nt}\\neq-1\\}q_{ntk}}\n",
    "$$\n",
    "*Write a function to calculate this frequency estimator given posterior weights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Matrix{Float64}:\n",
       " 0.46683824165885857\n",
       " 0.46903042606128603\n",
       " 0.46367854277239245\n",
       " 0.46850248045068527\n",
       " 0.4699661534000267"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j_observed_1 = zeros(10,1000)\n",
    "j_observed_1 .= J_data .== 1\n",
    "j_observed = zeros(10,1000)\n",
    "j_observed .= J_data .!= -1\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "function probs(α_obs, β_obs, Q_obs, P_guess, J_data, α_init, β_end)\n",
    "    #@show size(Q_obs)\n",
    "    P_out = zeros(5,1)\n",
    "    Q_new = iterate(α_obs, β_obs, Q_obs, P_guess, J_data, α_init, β_end)[3]\n",
    "    #@show size(Q_new)\n",
    "    #@show size(P_guess)\n",
    "    for k in 1:K\n",
    "        Q_in = Q_new[:,:,k]\n",
    "        num = j_observed_1 * Q_in\n",
    "        den = j_observed * Q_in\n",
    "        P_out[k] = sum(sum(num, dims = 1), dims = 2)[] ./ sum(sum(den, dims = 1), dims = 2)[]\n",
    "    end\n",
    "    return P_out\n",
    "end\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "out = probs(α_obs, β_obs, Q_obs, P_guess, J_data, α_init, β_end)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function updates our guess for the true parameters, $\\hat{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the E-M routine by iterating on this E-step and M-step until you get convergence in $\\hat{p}$. Does it look like you can recover the true parameters?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_init = [0.9; 0.9; 0.9; 0.9; 0.9;;]\n",
      "Iteration 10: error = 1.1102230246251565e-16\n",
      "Converged after 11 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5×1 Matrix{Float64}:\n",
       " 0.4663688497284882\n",
       " 0.4688451456796985\n",
       " 0.46296054173478446\n",
       " 0.4681993319114479\n",
       " 0.469577972593547"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function EM(J_data, α_init, β_end, toler, max_iter)\n",
    "\n",
    "    P_init = fill(0.9, K, 1)\n",
    "    @show P_init\n",
    "    α_iter = zeros(K,10,1000)\n",
    "    β_iter = zeros(K,10,1000)\n",
    "    Q_iter = zeros(K,10,1000)\n",
    "\n",
    "    error = 1.0\n",
    "   \n",
    "    for iter in 1:max_iter\n",
    "\n",
    "        α_iter, β_iter, Q_iter = iterate(α_iter, β_iter, Q_iter, P_init, J_data, α_init, β_end)\n",
    "        Q_iter = reshape(Q_iter, (5,10,1000))\n",
    "        P_update = probs(α_iter, β_iter, Q_iter, P_init, J_data, α_init, β_end)\n",
    "\n",
    "        error = maximum(abs, P_update - P_init)\n",
    "\n",
    "        if iter % 10 == 0\n",
    "            println(\"Iteration $iter: error = $error\")\n",
    "        end\n",
    "        \n",
    "        if error < toler && iter > 10 #### added iter > 10 only to avoid the error = 0.0 instantly which is not true\n",
    "            println(\"Converged after $iter iterations\")\n",
    "            return P_init\n",
    "        end\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        P_init = copy(P_update)\n",
    "\n",
    "    end\n",
    "\n",
    "    return P_init\n",
    "end\n",
    "\n",
    "P = EM(J_data, α_init, β_end, 1e-6, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fail to recover the true parameters, $\\hat{p}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
