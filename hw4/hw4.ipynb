{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ECON8502: Structural Microeconometrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Conor Bayliss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This week you will estimate a simple hidden Markov model using Expectation-Maximisation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here is the model. There is a discrete state variable $k\\in\\{1,2,3,...,K\\}$ and a binary outcome $j\\in\\{0,1\\}$ that:*\n",
    "1. *Is determined probabilistically by the state; and*\n",
    "2. *Moves the state up one grid point if $j=1$, i.e. $k_{t+1} = \\min\\{K,k_{t+j}\\}$*\n",
    "\n",
    "*Let $p$ be a K-dimensional vector where $p_k$ holds the probability that $j=1$ given that the model is in state $k$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The state $k$ is never observed and the outcome $j$ is only observed half the time (i.e. it is missing with probability 0.5). Thus, define $j^{*}$ to be:*\n",
    "$$\n",
    "j^* = \n",
    "\\begin{cases} \n",
    "j & \\text{with probability } 0.5 \\\\\n",
    "-1 & \\text{with probability } 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Our task is to estimate the vector of outcome probabilities $p$ non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below calcualtes $p$ and simulates panel data*\n",
    "$$\n",
    "(j^*_{nt})_{t=1,n=1}^{T,N}\n",
    "$$\n",
    "*To start with, assume $k_{n,1} = 1, \\forall n$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1000 Matrix{Float64}:\n",
       "  1.0   1.0  -1.0   0.0  -1.0   0.0  …   0.0   1.0  -1.0  -1.0  -1.0   1.0\n",
       "  1.0   0.0  -1.0   1.0   0.0   0.0      1.0  -1.0   1.0   1.0  -1.0  -1.0\n",
       "  1.0   0.0  -1.0   1.0   1.0  -1.0      0.0   1.0   1.0   1.0  -1.0   0.0\n",
       " -1.0   1.0   1.0  -1.0   0.0   1.0      1.0  -1.0  -1.0   0.0  -1.0  -1.0\n",
       " -1.0  -1.0  -1.0   0.0  -1.0  -1.0     -1.0   0.0  -1.0   1.0  -1.0  -1.0\n",
       "  0.0   1.0   1.0   0.0   1.0   0.0  …  -1.0  -1.0  -1.0  -1.0  -1.0   0.0\n",
       " -1.0  -1.0   0.0  -1.0   0.0  -1.0      1.0  -1.0   0.0  -1.0   0.0   0.0\n",
       "  1.0   0.0   0.0  -1.0  -1.0   0.0     -1.0   1.0  -1.0  -1.0  -1.0   1.0\n",
       "  0.0   1.0   0.0  -1.0  -1.0   0.0      0.0  -1.0   0.0  -1.0   0.0   0.0\n",
       " -1.0  -1.0   1.0  -1.0   0.0   0.0     -1.0  -1.0  -1.0  -1.0   0.0   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random, Distributions\n",
    "K = 5\n",
    "Pj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #<- choice probability as a function of k\n",
    "knext(k,j,K) = min(K,k+j) \n",
    "\n",
    "function simulate(N,T,Pj)\n",
    "    J = zeros(T,N)\n",
    "    K = length(Pj)\n",
    "    for n in axes(J,2)\n",
    "        k = 1\n",
    "        for t in axes(J,1)\n",
    "            j = rand()<Pj[k]\n",
    "            # record j probabilistically\n",
    "            if rand()<0.5\n",
    "                J[t,n] = -1\n",
    "            else\n",
    "                J[t,n] = j\n",
    "            end\n",
    "            # update state:\n",
    "            k = knext(k,j,K)\n",
    "        end\n",
    "    end\n",
    "    return J\n",
    "end\n",
    "\n",
    "J_data = simulate(1000,10,Pj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the outcomes $j$ are peridoically unobserved, there are two ways to set up the EM problem:*\n",
    "1. *Define a composite state variable $s=(k,j)$ that is partially unobserved and define $\\alpha$ and $\\beta$ over this composite state*\n",
    "2. *Define $\\alpha$ and $\\beta$ over $k$ only and sum over potential realisations of $j$ when missing*.\n",
    "\n",
    "*When the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since $j$ is binary, we will use the second approach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a function that (given a guess of the parameters $p$) takes a sequence of $(j^*)_{t=1}^T$ and runs the forward-back algorithm. In doing so, your function should fill in three objects:*\n",
    "1. *A $K$ x $T$ array of backward looking probabilities $(\\alpha)$*\n",
    "2. *A $K$ x $T$ array of forward looking probabilities $(\\beta)$*\n",
    "3. *A $K$ x $T$ array of posterior probabilities over each state $k(Q)$*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall that*\n",
    "$$\n",
    "\\alpha[k,s] = \\mathbb{P}[k_s=k,(j^*)_{t=1}^s]\n",
    "$$\n",
    "$$\n",
    "\\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^T|k_s=k]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "Q[k,s] = \\mathbb{P}[k_s=k|(j^*)_{t=1}^T]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a $K$ x $T$ x $N$ array of posterior weights.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take as given a set of posterior weights $q_{ntk}=Q[n,t,k]$. The expected log-likelihood for the M-step is:*\n",
    "$$\n",
    "\\mathcal{L}(p) = \\sum_n \\sum_t \\sum_k q_{ntk}(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_k)+\\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_k))\n",
    "$$\n",
    "*Show that the non-parametric maximum likelihood estimate of $p$ given the posterior weights is a frequency estimator:*\n",
    "$$\n",
    "\\hat{p_k} = \\frac{\\sum_n \\sum_t \\mathbf{1}\\{j^*_{nt}=1\\}q_{ntk}}{\\sum_n \\sum_t \\mathbf{1}\\{j^*_{nt}\\neq-1\\}q_{ntk}}\n",
    "$$\n",
    "*Write a function to calculate this frequency estimator given posterior weights.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the E-M routine by iterating on this E-step and M-step until you get convergence in $\\hat{p}$. Does it look like you can recover the true parameters?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
